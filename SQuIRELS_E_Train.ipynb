{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "from sys import stderr\n",
    "\n",
    "# for type hint\n",
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20\n",
    "gamma_max = 0.001\n",
    "gamma_min = 0.001\n",
    "\n",
    "\n",
    "suffix = '_GFlash_Energy'\n",
    "\n",
    "CUDA = True\n",
    "device = torch.device(\"cuda\" if CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://www.zijianhu.com/post/pytorch/ema/\n",
    "class EMA(nn.Module):\n",
    "    def __init__(self, model: nn.Module, decay: float):\n",
    "        super().__init__()\n",
    "        self.decay = decay\n",
    "\n",
    "        self.model = model\n",
    "        self.shadow = deepcopy(self.model)\n",
    "\n",
    "        for param in self.shadow.parameters():\n",
    "            param.detach_()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self):\n",
    "        if not self.training:\n",
    "            print(\"EMA update should only be called during training\", file=stderr, flush=True)\n",
    "            return\n",
    "\n",
    "        model_params = OrderedDict(self.model.named_parameters())\n",
    "        shadow_params = OrderedDict(self.shadow.named_parameters())\n",
    "\n",
    "        # check if both model contains the same set of keys\n",
    "        assert model_params.keys() == shadow_params.keys()\n",
    "\n",
    "        for name, param in model_params.items():\n",
    "            # see https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
    "            # shadow_variable -= (1 - decay) * (shadow_variable - variable)\n",
    "            shadow_params[name].sub_((1. - self.decay) * (shadow_params[name] - param))\n",
    "\n",
    "        model_buffers = OrderedDict(self.model.named_buffers())\n",
    "        shadow_buffers = OrderedDict(self.shadow.named_buffers())\n",
    "\n",
    "        # check if both model contains the same set of keys\n",
    "        assert model_buffers.keys() == shadow_buffers.keys()\n",
    "\n",
    "        for name, buffer in model_buffers.items():\n",
    "            # buffers are copied\n",
    "            shadow_buffers[name].copy_(buffer)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \n",
    "        if self.training:\n",
    "            return self.model(*args, **kwargs)\n",
    "        else:\n",
    "            return self.shadow(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = '/mnt/d/UFRGS/TCC/Dados/datasets/SB_Refinement/'\n",
    "file_path_gflash = folder_path + 'run_GFlash01_100k_10_100GeV_full'\n",
    "file_path_g4 = folder_path + 'run_Geant_100k_10_100GeV_full'\n",
    "file_name = '.npy'\n",
    "\n",
    "models_dir_path = '/mnt/d/UFRGS/TCC/Dados/repos/SB_refinement_models/'\n",
    "\n",
    "energy_voxel_g4 = np.load(file_path_g4 + file_name)[:, 0:100].astype(np.float32)\n",
    "energy_voxel_gflash  = np.load(file_path_gflash + file_name)[:, 0:100].astype(np.float32)\n",
    "\n",
    "energy_particle_g4 = np.load(file_path_g4 + file_name)[:, 200:201].astype(np.float32)/10000.0\n",
    "energy_particle_gflash  = np.load(file_path_gflash + file_name)[:, 200:201].astype(np.float32)/10000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by incident energy to define pairs\n",
    "mask_energy_particle_g4 = np.argsort(energy_particle_g4, axis=0)[:,0]\n",
    "mask_energy_particle_gflash = np.argsort(energy_particle_gflash, axis=0)[:,0]\n",
    "\n",
    "energy_particle_g4 = energy_particle_g4[mask_energy_particle_g4]\n",
    "energy_particle_gflash = energy_particle_gflash[mask_energy_particle_gflash]\n",
    "\n",
    "energy_voxel_g4 = energy_voxel_g4[mask_energy_particle_g4]\n",
    "energy_voxel_gflash = energy_voxel_gflash[mask_energy_particle_gflash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshuffle consistently\n",
    "mask_shuffle = np.random.permutation(energy_particle_g4.shape[0])\n",
    "\n",
    "energy_particle_g4 = energy_particle_g4[mask_shuffle]\n",
    "energy_particle_gflash = energy_particle_gflash[mask_shuffle]\n",
    "\n",
    "energy_voxel_g4 = energy_voxel_g4[mask_shuffle]\n",
    "energy_voxel_gflash = energy_voxel_gflash[mask_shuffle]\n",
    "\n",
    "\n",
    "energy_g4 = np.sum(energy_voxel_g4, 1, keepdims=True)\n",
    "energy_gflash = np.sum(energy_voxel_gflash, 1, keepdims=True)\n",
    "\n",
    "\n",
    "energy_voxel_g4 = np.reshape(energy_voxel_g4, (-1, 1, 10, 10))\n",
    "energy_voxel_gflash = np.reshape(energy_voxel_gflash, (-1, 1, 10, 10))\n",
    "\n",
    "\n",
    "energy_voxel_g4 = energy_voxel_g4/np.tile(np.reshape(energy_g4, (-1, 1, 1, 1)), (1, 1, 10, 10))\n",
    "energy_voxel_gflash = energy_voxel_gflash/np.tile(np.reshape(energy_gflash, (-1, 1, 1, 1)), (1, 1, 10, 10))\n",
    "\n",
    "\n",
    "energy_g4 = energy_g4/energy_particle_g4\n",
    "energy_gflash = energy_gflash/energy_particle_gflash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifter_g4 = np.mean(energy_voxel_g4, 0)\n",
    "shifter_gflash = np.mean(energy_voxel_gflash, 0)\n",
    "scaler_g4 = np.std(energy_voxel_g4, 0)\n",
    "scaler_gflash = np.std(energy_voxel_gflash, 0)\n",
    "\n",
    "energy_voxel_g4 = (energy_voxel_g4 - shifter_g4)/scaler_g4\n",
    "energy_voxel_gflash = (energy_voxel_gflash - shifter_gflash)/scaler_gflash\n",
    "\n",
    "\n",
    "\n",
    "shifter_energy_g4 = np.mean(energy_g4, 0)\n",
    "shifter_energy_gflash = np.mean(energy_gflash, 0)\n",
    "scaler_energy_g4 = np.std(energy_g4, 0)\n",
    "scaler_energy_gflash = np.std(energy_gflash, 0)\n",
    "\n",
    "energy_g4 = (energy_g4 - shifter_energy_g4)/scaler_energy_g4\n",
    "energy_gflash = (energy_gflash - shifter_energy_gflash)/scaler_energy_gflash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100000, 1])\n",
      "torch.Size([100000, 1])\n",
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1024*16\n",
    "\n",
    "\n",
    "npar = int(energy_voxel_g4.shape[0])\n",
    "\n",
    "            \n",
    "X_init = energy_gflash\n",
    "Y_init = energy_particle_gflash\n",
    "init_sample = torch.tensor(X_init)#.view(X_init.shape[0], 1, 10, 10)\n",
    "init_lable = torch.tensor(Y_init)\n",
    "init_ds = TensorDataset(init_sample, init_lable)\n",
    "init_dl = DataLoader(init_ds, batch_size=batch_size, shuffle=False)\n",
    "#init_dl = repeater(init_dl)\n",
    "print(init_sample.shape)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "X_final = energy_g4\n",
    "Y_final = energy_particle_g4\n",
    "final_sample = torch.tensor(X_final)#.view(X_final.shape[0], 1, 10, 10)\n",
    "final_label = torch.tensor(Y_final)\n",
    "final_ds = TensorDataset(final_sample, final_label)\n",
    "final_dl = DataLoader(final_ds, batch_size=batch_size, shuffle=False)\n",
    "#final_dl = repeater(final_dl)\n",
    "\n",
    "#mean_final = torch.tensor(0.)\n",
    "#var_final = torch.tensor(1.*10**3) #infty like\n",
    "\n",
    "mean_final = torch.zeros(final_sample.size(-1)).to(device)\n",
    "var_final = 1.*torch.ones(final_sample.size(-1)).to(device)\n",
    "\n",
    "print(final_sample.shape)\n",
    "print(mean_final.shape)\n",
    "print(var_final.shape)\n",
    "\n",
    "\n",
    "dls = {'f': init_dl, 'b': final_dl}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, layer_widths, activate_final = False, activation_fn=F.relu):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_width = input_dim\n",
    "        for layer_width in layer_widths:\n",
    "            layers.append(torch.nn.Linear(prev_width, layer_width))\n",
    "            # # same init for everyone\n",
    "            # torch.nn.init.constant_(layers[-1].weight, 0)\n",
    "            prev_width = layer_width\n",
    "        self.input_dim = input_dim\n",
    "        self.layer_widths = layer_widths\n",
    "        self.layers = torch.nn.ModuleList(layers)\n",
    "        self.activate_final = activate_final\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers[:-1]):\n",
    "            x = self.activation_fn(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        if self.activate_final:\n",
    "            x = self.activation_fn(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def get_timestep_embedding(timesteps, embedding_dim=128):\n",
    "    \"\"\"\n",
    "      From Fairseq.\n",
    "      Build sinusoidal embeddings.\n",
    "      This matches the implementation in tensor2tensor, but differs slightly\n",
    "      from the description in Section 3.5 of \"Attention Is All You Need\".\n",
    "      https://github.com/pytorch/fairseq/blob/master/fairseq/modules/sinusoidal_positional_embedding.py\n",
    "    \"\"\"\n",
    "    half_dim = embedding_dim // 2\n",
    "    emb = math.log(10000) / (half_dim - 1)\n",
    "    emb = torch.exp(torch.arange(half_dim, dtype=torch.float, device=timesteps.device) * -emb)\n",
    "\n",
    "    emb = timesteps.float() * emb.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
    "    if embedding_dim % 2 == 1:  # zero pad\n",
    "        emb = F.pad(emb, [0,1])\n",
    "\n",
    "    return emb\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ScoreNetwork(torch.nn.Module):\n",
    "    def __init__(self, encoder_layers=[256,256], pos_dim=128, decoder_layers=[256,256], x_dim=1, n_cond=1):\n",
    "\n",
    "        super().__init__()\n",
    "        self.temb_dim = pos_dim\n",
    "        t_enc_dim = pos_dim *2\n",
    "        self.locals = [encoder_layers, pos_dim, decoder_layers, x_dim]\n",
    "        self.n_cond = n_cond\n",
    "\n",
    "\n",
    "        self.net = MLP(3 * t_enc_dim + 1,\n",
    "                       layer_widths=decoder_layers +[x_dim],\n",
    "                       activate_final = False,\n",
    "                       activation_fn=torch.nn.LeakyReLU())\n",
    "\n",
    "        self.t_encoder = MLP(pos_dim,\n",
    "                             layer_widths=encoder_layers +[t_enc_dim],\n",
    "                             activate_final = False,\n",
    "                             activation_fn=torch.nn.LeakyReLU())\n",
    "\n",
    "        self.x_encoder = MLP(x_dim,\n",
    "                             layer_widths=encoder_layers +[t_enc_dim],\n",
    "                             activate_final = False,\n",
    "                             activation_fn=torch.nn.LeakyReLU())\n",
    "            \n",
    "        self.e_encoder = MLP(1,\n",
    "                             layer_widths=encoder_layers +[t_enc_dim],\n",
    "                             activate_final = False,\n",
    "                             activation_fn=torch.nn.LeakyReLU())\n",
    "        \n",
    "        \n",
    "    def forward(self, x, t, cond=None, selfcond=None):\n",
    "        if len(x.shape) == 1:\n",
    "            x = x.unsqueeze(0)\n",
    "\n",
    "        if len(selfcond.shape) == 1:\n",
    "            selfcond = selfcond.unsqueeze(0)\n",
    "\n",
    "        temb = get_timestep_embedding(t, self.temb_dim)\n",
    "        temb = self.t_encoder(temb)\n",
    "        \n",
    "        xemb = self.x_encoder(x)\n",
    "        \n",
    "            \n",
    "        if self.n_cond > 0:\n",
    "            eemb = cond\n",
    "            eemb = self.e_encoder(eemb)\n",
    "            h = torch.cat([xemb, temb, selfcond, eemb], -1)\n",
    "        else:\n",
    "            h = torch.cat([xemb ,temb, selfcond], -1)\n",
    "                \n",
    "        out = self.net(h) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010, 0.0010,\n",
      "        0.0010, 0.0010], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "lr = 1e-5\n",
    "\n",
    "n = num_steps//2\n",
    "gamma_half = np.linspace(gamma_min, gamma_max, n)\n",
    "gammas = np.concatenate([gamma_half, np.flip(gamma_half)])\n",
    "gammas = torch.tensor(gammas).to(device)\n",
    "T = torch.sum(gammas)\n",
    "\n",
    "print(gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1])\n",
      "torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "## decay=1.0: No change on update\n",
    "## decay=0.0: No memory of previous updates, memory is euqal to last update\n",
    "## decay=0.9: New value 9 parts previous updates, 1 part current update\n",
    "## decay=0.95: New value 49 parts previous updates, 1 part current update\n",
    "\n",
    "model_f = ScoreNetwork(n_cond = init_lable.size(1)).to(device)\n",
    "model_b = ScoreNetwork(n_cond = init_lable.size(1)).to(device)\n",
    "\n",
    "model_f = torch.nn.DataParallel(model_f)\n",
    "model_b = torch.nn.DataParallel(model_b)\n",
    "\n",
    "opt_f = torch.optim.Adam(model_f.parameters(), lr=lr)\n",
    "opt_b = torch.optim.Adam(model_b.parameters(), lr=lr)\n",
    "\n",
    "net_f = EMA(model=model_f, decay=0.95).to(device)\n",
    "net_b = EMA(model=model_b, decay=0.95).to(device)\n",
    "\n",
    "nets  = {'f': net_f, 'b': net_b }\n",
    "opts  = {'f': opt_f, 'b': opt_b }\n",
    "\n",
    "nets['f'].train()\n",
    "nets['b'].train()\n",
    "\n",
    "\n",
    "d = init_sample[0].shape  # shape of object to diffuse\n",
    "dy = init_lable[0].shape  # shape of object to diffuse\n",
    "print(d)\n",
    "print(dy)\n",
    "\n",
    "#print(net_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_gauss(x, m, var):\n",
    "    xout = (x - m) / var\n",
    "    return -xout\n",
    "\n",
    "\n",
    "def ornstein_ulhenbeck(x, gradx, gamma):\n",
    "    xout = x + gamma * gradx + \\\n",
    "        torch.sqrt(2 * gamma) * torch.randn(x.shape, device=x.device)\n",
    "    return xout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CacheLoader(Dataset):\n",
    "    def __init__(self, forward_or_backward = 'f', forward_or_backward_rev = 'b', first = False, sample = False):\n",
    "        super().__init__()\n",
    "        self.num_batches = int(npar/batch_size)\n",
    "\n",
    "        self.data = torch.zeros((self.num_batches, batch_size*num_steps, 2, *d)).to(device)  # .cpu()\n",
    "        self.y_data = torch.zeros((self.num_batches, batch_size*num_steps, *dy)).to(device)  # .cpu()\n",
    "        self.steps_data = torch.zeros((self.num_batches, batch_size*num_steps, 1)).to(device)  # .cpu() # steps\n",
    "\n",
    "\n",
    "\n",
    "        for b, dat in enumerate(dls[forward_or_backward]):    \n",
    "            #print(b, self.num_batches)\n",
    "            \n",
    "            if b == self.num_batches:\n",
    "                break\n",
    "\n",
    "            x = dat[0].float().to(device)\n",
    "            x_orig = x.clone().to(device)\n",
    "            y = dat[1].float().to(device)\n",
    "            steps = torch.arange(num_steps).to(device)\n",
    "            time = torch.cumsum(gammas, 0).to(device).float()\n",
    "\n",
    "\n",
    "            N = x.shape[0]\n",
    "            steps = steps.reshape((1, num_steps, 1)).repeat((N, 1, 1))\n",
    "            time = time.reshape((1, num_steps, 1)).repeat((N, 1, 1))\n",
    "            #gammas_new = gammas.reshape((1, num_steps, 1)).repeat((N, 1, 1))\n",
    "            steps = time\n",
    "\n",
    "            x_tot = torch.Tensor(N, num_steps, *d).to(x.device)\n",
    "            y_tot = torch.Tensor(N, num_steps, *dy).to(y.device)\n",
    "            out = torch.Tensor(N, num_steps, *d).to(x.device)\n",
    "            store_steps = steps\n",
    "            num_iter = num_steps\n",
    "            steps_expanded = time\n",
    "\n",
    "            with torch.no_grad():\n",
    "                if first:\n",
    "\n",
    "                    for k in range(num_iter):\n",
    "                        gamma = gammas[k]\n",
    "                        gradx = grad_gauss(x, mean_final, var_final)\n",
    "\n",
    "                        t_old = x + gamma * gradx\n",
    "                        z = torch.randn(x.shape, device=x.device)\n",
    "                        x = t_old + torch.sqrt(2 * gamma)*z\n",
    "                        gradx = grad_gauss(x, mean_final, var_final)\n",
    "\n",
    "                        t_new = x + gamma * gradx\n",
    "\n",
    "                        x_tot[:, k, :] = x\n",
    "                        y_tot[:, k, :] = y\n",
    "\n",
    "                        out[:, k, :] = (t_old - t_new)  # / (2 * gamma)\n",
    "\n",
    "\n",
    "                else:\n",
    "                    for k in range(num_iter):\n",
    "                        gamma = gammas[k]\n",
    "                        t_old = x + nets[forward_or_backward_rev](x, steps[:, k, :], y, x_orig)\n",
    "\n",
    "                        if sample & (k == num_iter-1):\n",
    "                            x = t_old\n",
    "                        else:\n",
    "                            z = torch.randn(x.shape, device=x.device)\n",
    "                            x = t_old + torch.sqrt(2 * gamma) * z\n",
    "                        t_new = x + nets[forward_or_backward_rev](x, steps[:, k, :], y, x_orig)\n",
    "\n",
    "                        x_tot[:, k, :] = x\n",
    "                        y_tot[:, k, :] = y\n",
    "                        \n",
    "                        \n",
    "                        out[:, k, :] = (t_old - t_new)\n",
    "\n",
    "                x_tot = x_tot.unsqueeze(2)\n",
    "                out = out.unsqueeze(2)\n",
    "\n",
    "                batch_data = torch.cat((x_tot, out), dim=2)\n",
    "                flat_data = batch_data.flatten(start_dim=0, end_dim=1)\n",
    "                self.data[b] = flat_data\n",
    "                \n",
    "                \n",
    "                y_tot = y_tot.unsqueeze(1)\n",
    "                \n",
    "                flat_y_data = y_tot.flatten(start_dim=0, end_dim=1)\n",
    "                self.y_data[b] = flat_y_data.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "\n",
    "                flat_steps = steps_expanded.flatten(start_dim=0, end_dim=1)\n",
    "                self.steps_data[b] = flat_steps\n",
    "\n",
    "        self.data = self.data.flatten(start_dim=0, end_dim=1)\n",
    "        self.y_data = self.y_data.flatten(start_dim=0, end_dim=1)\n",
    "        self.steps_data = self.steps_data.flatten(start_dim=0, end_dim=1)\n",
    "\n",
    "        print('Cache size: {0}'.format(self.data.shape))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        item = self.data[index]\n",
    "        x = item[0]\n",
    "        out = item[1]\n",
    "        steps = self.steps_data[index]\n",
    "        y = self.y_data[index]\n",
    "        \n",
    "        return x, out, y, steps\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_ipf(n_iter = 200, forward_or_backward = 'f', forward_or_backward_rev = 'b', first = False, sample = False):\n",
    "                    \n",
    "    CL = CacheLoader(forward_or_backward, forward_or_backward_rev, first, sample)\n",
    "    CL = DataLoader(CL, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for i_iter in range(n_iter):\n",
    "\n",
    "        for (i, data_iter) in enumerate(CL):\n",
    "            (x, out, y, steps_expanded) = data_iter\n",
    "            x = x.to(device)\n",
    "            x_orig = x.clone().to(device)\n",
    "            y = y.to(device)\n",
    "            out = out.to(device)\n",
    "            steps_expanded = steps_expanded.to(device)\n",
    "            eval_steps = T - steps_expanded\n",
    "\n",
    "\n",
    "            pred = nets[forward_or_backward](x, eval_steps, y, x_orig)\n",
    "\n",
    "            loss = F.mse_loss(pred, out)\n",
    "            loss.backward()\n",
    "    \n",
    "            #print(loss)\n",
    "    \n",
    "            opts[forward_or_backward].step()\n",
    "            opts[forward_or_backward].zero_grad()\n",
    "            \n",
    "        print(loss)\n",
    "        #EMA update\n",
    "        nets[forward_or_backward].update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iter=0\n",
    "\n",
    "for i in range(1, 100):\n",
    "    try:\n",
    "        nets['f'].load_state_dict(torch.load(models_dir_path + 'Iter{:d}_net_f'.format(i) + suffix + '.pth', map_location=device))\n",
    "        nets['b'].load_state_dict(torch.load(models_dir_path + 'Iter{:d}_net_b'.format(i) + suffix + '.pth', map_location=device))\n",
    "        \n",
    "        start_iter = i\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "n_iter_glob = 50\n",
    "\n",
    "if start_iter == 0:  \n",
    "    iterate_ipf(n_iter = 100, forward_or_backward = 'f', forward_or_backward_rev = 'b', first = True)\n",
    "    print('--------------- Done iter 0 ---------------')\n",
    "\n",
    "\n",
    "nets['f'].train()\n",
    "nets['b'].train()\n",
    "\n",
    "for i in range(start_iter+1, start_iter+20):\n",
    "\n",
    "    iterate_ipf(n_iter = n_iter_glob, forward_or_backward = 'b', forward_or_backward_rev = 'f', first = False)\n",
    "    print('--------------- Done iter B{:d} ---------------'.format(i))\n",
    "\n",
    "    iterate_ipf(n_iter = n_iter_glob, forward_or_backward = 'f', forward_or_backward_rev = 'b', first = False)\n",
    "    print('--------------- Done iter F{:d} ---------------'.format(i))\n",
    "\n",
    "    torch.save(net_f.state_dict(), models_dir_path + 'Iter{:d}_net_f'.format(i) + suffix + '.pth')\n",
    "    torch.save(net_b.state_dict(), models_dir_path + 'Iter{:d}_net_b'.format(i) + suffix + '.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sb_ref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
